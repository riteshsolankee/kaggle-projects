---
title: "PersonalizationMedicineClassification"
author: "Ritesh Kumar"
date: "7/20/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

## Deleting all variables
```{r clear workspace}
rm(list=ls())
```

## load libraries
```{r}
library(data.table)
library(Matrix)
library(xgboost)
library(caret)
library(stringr)
library(tm)
library(syuzhet) 
library(dplyr)
library(ggplot2)
```

## LabelCount Encoding function
```{r}
# LabelCount Encoding function
labelCountEncoding <- function(column){
  return(match(column,levels(column)[order(summary(column,maxsum=nlevels(column)))]))
}

```

## Loading the datafiles
```{r}
# Load CSV files
cat("Read data")
train_text <- do.call(rbind,strsplit(readLines(unz("data/training_text.zip", "training_text")),'||',fixed=T))
train_text <- as.data.table(train_text)
train_text <- train_text[-1,]
colnames(train_text) <- c("ID", "Text")
train_text$ID <- as.numeric(train_text$ID)

test_text <- do.call(rbind,strsplit(readLines(unz('data/test_text.zip','test_text')),'||',fixed=T))
test_text <- as.data.table(test_text)
test_text <- test_text[-1,]
colnames(test_text) <- c("ID", "Text")
test_text$ID <- as.numeric(test_text$ID)

train <- fread("data/training_variants", sep=",", stringsAsFactors = T)
test <- fread("data/test_variants", sep=",", stringsAsFactors = T)
train <- merge(train,train_text,by="ID")
test <- merge(test,test_text,by="ID")
rm(test_text,train_text);gc()

test$Class <- -1
data <- rbind(train,test)
rm(train,test);gc()

```

## Data processing 
```{r}
# Basic text features
cat("Basic text features")
data$nchar <- as.numeric(nchar(data$Text))
data$nwords <- as.numeric(str_count(data$Text, "\\S+"))

# TF-IDF
cat("TF-IDF")
txt <- Corpus(VectorSource(data$Text))
txt <- tm_map(txt, stripWhitespace)
txt <- tm_map(txt, content_transformer(tolower))
txt <- tm_map(txt, removePunctuation)
txt <- tm_map(txt, removeWords, stopwords("english"))
txt <- tm_map(txt, stemDocument, language="english")
txt <- tm_map(txt, removeNumbers)
dtm <- DocumentTermMatrix(txt, control = list(weighting = weightTfIdf))
dtm <- removeSparseTerms(dtm, 0.95)
data <- cbind(data, as.matrix(dtm))

# LabelCount Encoding for Gene and Variation
# We can do more advanced feature engineering later, e.g. char-level n-grams
data$Gene <- labelCountEncoding(data$Gene)
data$Variation <- labelCountEncoding(data$Variation)

# Sentiment analysis
cat("Sentiment analysis")
sentiment <- get_nrc_sentiment(data$Text) 
data <- cbind(data,sentiment) 

# Set seed
set.seed(2016)
cvFoldsList <- createFolds(data$Class[data$Class > -1], k=5, list=TRUE, returnTrain=FALSE)

# To sparse matrix
cat("Create sparse matrix")
varnames <- setdiff(colnames(data), c("ID", "Class", "Text"))
train_sparse <- Matrix(as.matrix(sapply(data[Class > -1, varnames, with=FALSE],as.numeric)), sparse=TRUE)
test_sparse <- Matrix(as.matrix(sapply(data[Class == -1, varnames, with=FALSE],as.numeric)), sparse=TRUE)
y_train <- data[Class > -1,Class]-1
test_ids <- data[Class == -1,ID]
dtrain <- xgb.DMatrix(data=train_sparse, label=y_train)
dtest <- xgb.DMatrix(data=test_sparse)
```


## Intermideate results
```{r}
str(data)

#Sparce Matrix
str(train_sparse)
str(test_sparse)

## Dense Matrix
str(dtrain)
str(dtest)
```
## Prediction
```{r}
# Params for xgboost
param <- list(booster = "gbtree",
              objective = "multi:softprob",
              eval_metric = "mlogloss",
              num_class = 9,
              eta = .2,
              gamma = 1,
              max_depth = 5,
              min_child_weight = 1,
              subsample = .7,
              colsample_bytree = .7
)
```

```{r}
# Cross validation - determine CV scores & optimal amount of rounds
cat("XGB cross validation")
xgb_cv <- xgb.cv(data = dtrain,
                 params = param,
                 nrounds = 1000,
                 maximize = FALSE,
                 prediction = TRUE,
                 folds = cvFoldsList,
                 print_every_n = 5,
                 early_stopping_rounds = 100
)

```
```{r}
rounds <- which.min(xgb_cv$evaluation_log[, test_mlogloss_mean])
rounds
```


```{r}
# Train model
cat("XGB training")
xgb_model <- xgb.train(data = dtrain,
                       params = param,
                       watchlist = list(train = dtrain),
                       nrounds = rounds,
                       verbose = 1,
                       print_every_n = 5
)
```

```{r}
# Feature importance
cat("Plotting feature importance")
names <- dimnames(train_sparse)[[2]]
importance_matrix <- xgb.importance(names,model=xgb_model)
xgb.plot.importance(importance_matrix[1:60,],10)
```

```{r}

# Predict and output csv
cat("Predictions")
preds <- as.data.table(t(matrix(predict(xgb_model, dtest), nrow=9, ncol=nrow(dtest))))
colnames(preds) <- c("class1","class2","class3","class4","class5","class6","class7","class8","class9")
write.table(data.table(ID=test_ids, preds), "data/submission1.csv", sep=",", dec=".", quote=FALSE, row.names=FALSE)
```

## Result Analysis
```{r}

OOF_prediction <- data.frame(preds) %>%
  mutate(max_prob = max.col(., ties.method = "last"))
head(OOF_prediction)


str(OOF_prediction$max_prob)

OOF_prediction %>%
  group_by(max_prob) %>%
  summarise(ct = n())  %>%
  arrange(desc(ct)) %>%
  ggplot(aes(x=reorder(max_prob, -ct), y=ct, fill=ct)) +
  geom_bar(stat = 'identity') +
  ggtitle("Prediction - Class target distribution\n", subtitle="Classes Vs  Count") + 
  xlab("\n Class") + 
  ylab("\n Count") 

```


